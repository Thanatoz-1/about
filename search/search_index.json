{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Tushar Dhyani","text":"<p> <p>I'm research scientist at Sony Europe B.V. (since 2024). I have completed my Master's in Computational Linguistics from University of Stuttgart (2021-2023). My research focuses on advancing speech processing, speech restoration, and addressing audio inverse problems and their evaluation methods. I am particularly passionate about leveraging Generative AI to tackle challenges in speech and language technology.</p> Download Resume View my work <p></p>"},{"location":"blog/","title":"My Projects","text":""},{"location":"blog/2023/01/31/bengali-handwritten-grapheme-detection/","title":"Bengali Handwritten Grapheme detection","text":""},{"location":"blog/2023/01/31/bengali-handwritten-grapheme-detection/#the-problem","title":"The problem","text":"<p>  Automatic handwritten character recognition (HCR) and optical character recognition (OCR) are quite popular for commercial and academic reasons. For alpha-syllabary languages this problem increases manifolds due to its non-linear structure. Bengali, a member of alpha-syllabary family, is way trickier than English as it has 50 letters - 11 vowels and 39 consonants - plus 18 diacritics. This means there are roughly 13,000 ways to write Bengali letters, whereas English only has about 250 ways to do the same. This huge number of combinations makes recognizing Bengali characters a lot harder. These different elements has been shown below for a visual understanding. </p> <p> </p> <p> </p> <p>Different vowel diacritics (green) and consonant diacritics (red) used in Bengali orthography. The placement of the diacritics are not dependent on the grapheme root.</p> <p>we have the following distribution of graphemes in Bengali.  <pre><code>Number of unique grapheme roots: 168\nNumber of unique vowel diacritic: 11\nNumber of unique consonant diacritic: 7\n</code></pre></p>"},{"location":"blog/2023/01/31/bengali-handwritten-grapheme-detection/#solutions","title":"Solutions","text":"<p>To create a mixture of different roots, I followed a unique augmentation technique - cut-mix. This technique involves randomly cutting and mixing parts of different images while training. But, as a native speaker of the same family of language, I knew that there were some specific ways to achieve this. This way involves having specific class in a specific zone. From this, I devised this zone structure and did a cut-mix augmentation while training my model. The specific zones are showcased below:</p> <p> </p> <p>Based on the rough distribution of the zone, I mixed respective parts to create augmentations that sometimes resembled real and sometimes unreal examples. The process could be understood from the picture below how this structure helped increase the training data distribution. </p> <p> </p> <p>Sadly this process didn't work directly, because the mistakes and unrealistic images are too much noise for the model to deal with. I was able to tone down the noise by doing the following tricks - The grapheme root zones are roughly correct most of the time, while the vowel and consonants are way off. I figured that since grapheme root and consonant 3 and 6 are the main classes to tackle, I should focus on those.</p> <p>My training pipeline roughly looked as follows:  </p>"},{"location":"blog/2023/01/31/bengali-handwritten-grapheme-detection/#results","title":"Results","text":"<p>Models are evaluated using a hierarchical macro-averaged recall. First, a standard macro-averaged recall is calculated for each component (grapheme root, vowel diacritic, or consonant diacritic). The final score is the weighted average of those three scores, with the grapheme root given double weight.</p> <p> Table 1: Results of different model that I used for training Model name Validation score Public score Private score Root Vowel Dicritic Consonant Dicritic Overall EfficientNet B1 0.971 0.988 0.978 0.977 0.9638 0.9385 EfficientNet B2 0.921 0.978 0.987 0.952 0.9615 0.9245 Resnet18 0.951 0.982 0.978 0.966 0.9599 0.9223 <p></p>"},{"location":"blog/2023/01/31/bengali-handwritten-grapheme-detection/#score-on-the-leaderboard","title":"Score on the leaderboard","text":"<p>With the best backbone model that I had, I scored 55 on the private leaderboard out of 2060 participants. This got me my first silver medal on Kaggle.  </p>"},{"location":"blog/2022/02/21/denoising-diffusion-for-speech-enhancement/","title":"Denoising diffusion for speech enhancement","text":"<p>This research has been made available on Researchgate. You can also send me a request over personal email to access a pdf copy of the report if you encounter some issues with the link above.</p>"},{"location":"blog/2022/02/21/denoising-diffusion-for-speech-enhancement/#introduction","title":"Introduction","text":"<p>Audio recordings in the real world are typically tainted by noise and other distortions. These distortions come from many factors such as environmental noises, and distortions from various kinds of electronics, circuits, and microphones. These noises and distortions cause issues during the perception of speech to the receiver. This project tries to provide a solution to the problem of noise in speech using a generative approach and compares the effectiveness of the generative approach over other existing approaches.</p> <p></p> <p>A recent outbreak of generative models has pushed their capacity to generate high-fidelity data and one crucial class of such generative models is Diffusion models. Diffusion models, inspired by thermodynamic diffusion, are generative models that learn generation by learning to denoise diffused images. This could be an oversimplified explanation of an extremely convoluted approach in generative models. To understand diffusion, try to comprehend the above image. </p>"},{"location":"blog/2022/02/21/denoising-diffusion-for-speech-enhancement/#how-diffusion-works","title":"How diffusion works.","text":"<p>Let's start by looking into the image and avoiding all the surrounding text and maths. What you see on the farthest left is a spectrogram of speech. (If you are unaware of what a spectrogram is, try reading about it or watching this simple video). For simplicity, let us consider the spectrogram to be an image or maybe simply consider any image in its place. As we move from the left end to the right, we see that noise is getting added to the image at every step. This process of adding noise is called diffusion. Diffusion destroys the image at every step by adding more and more noise. We train a model to learn to remove this induced noise from the image and generate the real image again. This process of removing noise is denoising. When the number of steps of diffusion and denoising are increased and a model is trained for a really large number of steps, it becomes better and better at this.</p>"},{"location":"blog/2022/02/21/denoising-diffusion-for-speech-enhancement/#how-this-project-solves-the-issue","title":"How this project solves the issue","text":"<p>We saw how diffusion models work above. If we relate the diffusion process to the actual noise introduction in the real world in audio files or speech signals, we can understand the core idea behind this project. We train a diffusion model to learn denoising the spectrograms and generate the speech signals well. This cannot be achieved without conditioning the spectrograms on real-world images.</p>"},{"location":"blog/2022/02/21/denoising-diffusion-for-speech-enhancement/#what-are-the-application","title":"What are the application","text":"<p>During the rise of virtual meetings and conferences, all of us have understood the importance of clean speech. These generative networks could play a vital role in receiving high-fidelity speech signals on the receiver's end and have completely noise-free communication. Apart from that, they can also play a critical role in systems generating audio output such as Text-to-speech and give high-fidelity output. One such pipeline has been shown in the image below:</p> <p></p> <p>This research is still underway. To read more about it and access my early report, you can send me a request over email.</p>"},{"location":"blog/2023/02/21/help-im-trapped-in-a-universe-factory/","title":"Investigating Semantic Roles for Emotion Role Prediction","text":"<p>A little detailed scientific report on this project is available on Researchgate. Please feel free to read and share your feedback with me over email</p> <p>Emotion analysis primarily focuses on classifying, predicting and retrieving emotions and their related properties from text. However, only few research was conducted towards analyzing the semantic roles of emotions, i.e. who is experiencing which emotion, what caused it and what or whom is it directed towards. This project investigate the influence of semantic role labels on emotion role prediction. Building on top of previous approaches and resources, I've implemented a framework for predicting emotion roles using different features with co-researcher Maximilian Wegge. We find that semantic role label features have no significant influence on the task and identify two possible reasons for that. </p> <p>This project was conducted under the supervision of Dr. Roman Klinger</p>"},{"location":"blog/2023/02/21/help-im-trapped-in-a-universe-factory/#what-is-semantic-role-labelling","title":"What is Semantic Role Labelling?","text":"<p>In natural languages, we understand the who, why, which, what, etc. in a sentence by understanding the words and phrases and their semantic meaning. This semantic meaning helps a lot in understanding why emotion is triggered, towards whom it is directed, or what gets affected. The process of pointing these individual components from a sentence is called Semantic Role Labelling or is sometimes analogous to Emotion Role labeling.</p> <p>Semantic role labeling provides us with fine-grained control over emotion classification or prediction as we get a clear understanding of it. </p> <p>Here, who (experiencer) feels which emotion (indicated by cue), which object, person, or instance the emotion is directed towards (target), and what evoked the emotion in the feeler (cause).</p>"},{"location":"blog/2023/02/21/help-im-trapped-in-a-universe-factory/#datasets-used","title":"Datasets used:","text":"<ul> <li>Reman</li> <li>Good News Everyone (GNE)</li> <li>Emotion stimuli</li> <li>Electoral Tweets</li> <li>Emotion Cause Analysis (ECA)</li> </ul>"},{"location":"blog/2023/02/21/help-im-trapped-in-a-universe-factory/#methods","title":"Methods","text":""},{"location":"blog/2023/02/21/help-im-trapped-in-a-universe-factory/#metric","title":"Metric","text":"<p>We evaluate our approach by calculating the Jaccard value and reporting the F1 score of the overlapping spans. The Jaccard index is defined as $$ J(A,B) = \\frac{| A \\cap B |}{| A \\cup B |} = \\frac{| A \\cap B|}{|A| + |B| - |A \\cap B|} $$</p> <p>In our case, we take A as our prediction span and B as the target span. We calculate a span to be correct only if the predicted and the target span have 80% overlap.</p>"},{"location":"blog/2023/02/21/help-im-trapped-in-a-universe-factory/#results","title":"Results","text":"<p>We run our experiments on the testing split of the following corpora and compare the performances using jaccard score. For our evaluation, we set the overlap threshold to 0.8 and consider only the spans that have atleast 80% overlap compared to the training spans. </p> <p> Dataset Method Exp Tar Cue Cause REMAN HMM 0.228 0.021 - 0.011 biLSTM-emb 0.435 0.051 - 0.101 biLSTM-emb+srl(all) 0.494 0.025 - 0.078 biLSTM-emb+srl(slct) 0.465 0.115 - 0.139 Table 1: Demonstrate the results of our experiments on Reman corpus. Our model achieves higher scores compared to our baseline using HMM model. Dataset Method Exp Tar Cue Cause GNE HMM 0.370 0.062 0.293 0.321 biLSTM-emb 0.595 0.228 0.441 0.654 biLSTM-emb+srl(all) 0.591 0.312 0.443 0.638 biLSTM-emb+srl(slct) 0.553 0.278 0.408 0.613 Table 2: The results of our experiments on Good News Everyone corpus. Our model's performance still remains higher compared to baseline HMM model. Dataset Method Exp Tar Cue Cause ES HMM - - - 0.215 biLSTM-emb - - - 0.593 biLSTM-emb+srl(all) - - - 0.591 biLSTM-emb+srl(slct) - - - 0.553 Table 3: Results of our model on Emotion Stimuli dataset for cause only. The results show that SRL features do not really impact the performance on respective corpus. Dataset Method Exp Tar Cue Cause ET HMM 0.0 0.228 0.122 0.124 biLSTM-emb 0.0 0.383 0.170 0.047 biLSTM-emb+srl(all) 0.0 0.434 0.134 0.085 biLSTM-emb+srl(slct) 0.0 0.443 0.136 0.070 Table 4: Results of our model on Electoral Tweet dataset. The results still suggests that the model does not achieve any improvement using the SRL features. Dataset Method Exp Tar Cue Cause ECA HMM - - - 0.025 biLSTM-emb - - - 0.155 biLSTM-emb+srl(all) - - - 0.206 biLSTM-emb+srl(slct) - - - 0.152  Table 5: Results demonstrate no improvement using the SRL all and selected features in improvement of emotion detection. <p></p> <p>A detailed scientific report on this project is available on Researchgate. Please feel free to read and share your feedback with me over email</p>"},{"location":"blog/2018/07/15/jigsaw-toxicity-detection/","title":"Jigsaw toxicity detection","text":""},{"location":"blog/2018/07/15/jigsaw-toxicity-detection/#problem","title":"Problem","text":"<p>A primary focus lies in developing machine learning models capable of detecting toxicity within online discussions. Toxicity, in this context, refers to anything perceived as rude, disrespectful, or potentially causing someone to exit a conversation. Typically, toxicity is categorized using binary classification, but this approach limits the ability to discern the severity of toxic comments. In my project for a Kaggle competition, I present a system aimed at addressing this limitation.</p>"},{"location":"blog/2018/07/15/jigsaw-toxicity-detection/#method","title":"Method","text":"<p>My final solution was weighted average of 8 models:</p> <ul> <li>BERT Base Uncased</li> <li>BERT Base Cased</li> <li>BERT Large Uncased</li> <li>BERT Large Cased</li> <li>BERT Large Uncased WWM</li> <li>BERT Large Cased WWM</li> <li>BiLSTM with FastText and Glove</li> <li>GPT-2 Large</li> </ul> <p>Data: I trained all final models except LSTM on 95% of data, leaving 5% as cold holdout for validation. Final LSTM was trained on full dataset. For my LSTM I used standard cleaning and preprocessing. All other models were trained on RAW data: no cleaning, no preprocessing, no feature engineering. Well, I guess future is now?</p> <p>Loss: I used weighted sigmoid cross-entropy loss (torch.nn.BCEWithLogitsLoss) with different sets of weights and target variants (including 6 additional toxicity subtypes).</p> <p>The training and inference pipelines for my model training is as follows:</p> <p></p>"},{"location":"blog/2018/07/15/jigsaw-toxicity-detection/#results","title":"Results","text":""},{"location":"blog/2023/05/15/sound-demixing/","title":"Sound demixing","text":""},{"location":"blog/2023/05/15/sound-demixing/#problem","title":"Problem:","text":"<p>Separating different sounds in a recording is called sound separation. In this project, I focused on just two uses: music and movies. When it comes to music, sound separation means pulling out voices and instruments from a song. For example, let's break down the parts of two songs that I found randomly. Thanks to the creators.</p> <p> </p> <p>This project was a part of Sound demixing challenge organized by Sony on AIcrowd. The report to this challenge is available here.</p> Source Sound demixing sample Sound demixing sample Mixture Vocals Drums Bass <p>The challenge was broadly divided into two tracks: Music demixing (MDX) and Cinematic sound demixing (CDX) but the aim was still similar. The complexity of the problem lay in sub-problems: Labelling noise (leaderboard A), Bleeding in recording (Leaderboard B) and General source separation (Leaderboard C). But due to contraints with time and training resources, I decided to stay with the general source separation.  </p>"},{"location":"blog/2023/05/15/sound-demixing/#method","title":"Method","text":"<p>This was one of the most interesting challenges that I worked on in some time. The challenge here was not to find the available research as there were quite a few that I was already aware of, but to find resources to train the model. For this, the university student resources were very helpful, so thanks to the university student body for that.</p> <p>So, for the challenge, I decided to train the availble models with some modification that I will be elaborating later. But overall the idea was to find an optimal way to train the models for each track. </p> <p>I started infering the available pretrained models. For my experiments I used the following available resources.</p> <ul> <li>Demucs v{1,2,3,4}</li> <li>MDXnet</li> <li>ByteSep</li> <li>Danna-sep</li> </ul> <p>Based on a my validation dataset, I found ByteSep to be the perfect candidate for training.</p> <p>After checking for data consistency, I </p>"},{"location":"blog/2023/05/15/sound-demixing/#rankings","title":"Rankings:","text":"<p>So after the challenge, the final standing of my model were as follows for each track</p> <p> <p></p>"},{"location":"blog/archive/2023/","title":"2023","text":""},{"location":"blog/archive/2022/","title":"2022","text":""},{"location":"blog/archive/2018/","title":"2018","text":""},{"location":"blog/category/competitions/","title":"Competitions","text":""},{"location":"blog/category/audio/","title":"Audio","text":""},{"location":"blog/category/research/","title":"Research","text":""},{"location":"blog/category/text/","title":"Text","text":""},{"location":"blog/category/vision/","title":"Vision","text":""}]}